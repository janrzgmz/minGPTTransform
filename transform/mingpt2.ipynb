{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1ada7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from transform.model_tce import GPT as GPT_tce\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915b2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds & Hyperparameters\n",
    "\n",
    "set_seed(3407)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_type = 'gpt2-medium' # 345M parameters\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 1024\n",
    "learning_rate = 3e-4\n",
    "max_iters = 20001\n",
    "batch_size = 512\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b788e4f",
   "metadata": {},
   "source": [
    "GPT2 weights from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984c47c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 354.82M\n",
      "⚠️ Warning: transformer.h.0.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.1.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.2.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.3.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.4.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.5.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.6.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.7.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.8.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.9.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.10.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.11.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.12.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.13.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.14.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.15.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.16.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.17.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.18.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.19.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.20.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.21.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.22.attn.bias not in HF checkpoint, skipping\n",
      "⚠️ Warning: transformer.h.23.attn.bias not in HF checkpoint, skipping\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mArtificial intelligence in modern age\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(prompt, num_samples, steps, do_sample)\u001b[39m\n\u001b[32m     12\u001b[39m x = encoded_input[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m x = x.expand(num_samples, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m     16\u001b[39m     out = tokenizer.decode(y[i].cpu().squeeze())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gptTransform/minGPTTransform/mingpt/model.py:292\u001b[39m, in \u001b[36mGPT.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens, temperature, do_sample, top_k)\u001b[39m\n\u001b[32m    290\u001b[39m idx_cond = idx \u001b[38;5;28;01mif\u001b[39;00m idx.size(\u001b[32m1\u001b[39m) <= \u001b[38;5;28mself\u001b[39m.block_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, -\u001b[38;5;28mself\u001b[39m.block_size:]\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m logits, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[32m    294\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :] / temperature\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gptTransform/minGPTTransform/mingpt/model.py:270\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    268\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.drop(tok_emb + pos_emb)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.h:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.ln_f(x)\n\u001b[32m    272\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gptTransform/minGPTTransform/mingpt/model.py:91\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.mlpf(\u001b[38;5;28mself\u001b[39m.ln_2(x))\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/apps/anaconda3/envs/jan-gptTransform-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gptTransform/minGPTTransform/mingpt/model.py:62\u001b[39m, in \u001b[36mCausalSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     59\u001b[39m v = v.view(B, T, \u001b[38;5;28mself\u001b[39m.n_head, C // \u001b[38;5;28mself\u001b[39m.n_head).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m att = (\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m) * (\u001b[32m1.0\u001b[39m / math.sqrt(k.size(-\u001b[32m1\u001b[39m)))\n\u001b[32m     63\u001b[39m att = att.masked_fill(\u001b[38;5;28mself\u001b[39m.bias[:,:,:T,:T] == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     64\u001b[39m att = F.softmax(att, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(model_type)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "    if prompt == '':\n",
    "        prompt = '<|endoftext|>'\n",
    "    encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    x = encoded_input['input_ids']\n",
    "    x = x.expand(num_samples, -1)\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "\n",
    "generate(prompt='Artificial intelligence in modern age', num_samples=10, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87a2f5",
   "metadata": {},
   "source": [
    "Training minGPT, model_type: gpt2-medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42) # 90% training, 10% validation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': ''})\n",
    "\n",
    "print('train_dataset length: ', len(train_dataset))\n",
    "print('val_dataset length: ', len(val_dataset))\n",
    "print('vocab_size: ', tokenizer.vocab_size)\n",
    "print(\"train_dataset example:\\n\", train_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "\n",
    "tokenizer.model_max_length = int(1e9)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer([t + tokenizer.eos_token for t in examples[\"text\"]], add_special_tokens=False,  return_attention_mask=False)\n",
    "\n",
    "def group_texts_for_minGPT(examples):\n",
    "  concatenated = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "  window = block_size + 1\n",
    "  total_length = (len(concatenated) // window) * window\n",
    "  if total_length == 0:\n",
    "      return {\"input_ids\": [], \"labels\": []}\n",
    "  chunks = [concatenated[i:i+window] for i in range(0, total_length, window)]\n",
    "  inputs = [c[:-1] for c in chunks]\n",
    "  labels = [c[1:]  for c in chunks]\n",
    "  return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['id', 'url', 'title', \"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['id', 'url', 'title', \"text\"])\n",
    "\n",
    "lm_train = tokenized_train_dataset.map(group_texts_for_minGPT, batched=True)\n",
    "lm_val = tokenized_val_dataset.map(group_texts_for_minGPT, batched=True)\n",
    "\n",
    "lm_train.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "lm_val.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wikiDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        x = item[\"input_ids\"]\n",
    "        y = item[\"labels\"]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an example instance of the dataset\n",
    "\n",
    "train_dataset = wikiDataset(lm_train)\n",
    "val_dataset = wikiDataset(lm_val)\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "a, b = val_dataset[0]\n",
    "\n",
    "print('x:\\n', x)\n",
    "print('y:\\n', y)\n",
    "print('a:\\n', a)\n",
    "print('b:\\n', b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True,  batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Determine val_loss and perplexity during training\n",
    "def evaluate(model, loader):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float('inf')\n",
    "    if was_training:\n",
    "      model.train()\n",
    "    return avg_loss, ppl\n",
    "\n",
    "# Generate text during training\n",
    "def generate_with_model(model, tokenizer, prompt, steps=50, num_samples=1):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    x = encoded_input['input_ids'].expand(num_samples, -1)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens=steps, do_sample=True, top_k=40)\n",
    "    outputs = []\n",
    "    for i in range(num_samples):\n",
    "        outputs.append(tokenizer.decode(y[i].cpu().squeeze()))\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4cd8b",
   "metadata": {},
   "source": [
    "Train ordinary minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = tokenizer.vocab_size\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = learning_rate\n",
    "train_config.max_iters = max_iters\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = num_workers\n",
    "\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_mingpt = []\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 200 == 0:\n",
    "      train_loss = trainer.loss.item()\n",
    "      train_losses_mingpt.append(train_loss)\n",
    "      perplexity = math.exp(train_loss) if train_loss < 20 else float('inf')\n",
    "      print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {train_loss:.5f}, perplexity {perplexity:.2f}\")\n",
    "    if trainer.iter_num % 2000 == 0:\n",
    "      # val_loss and val_perplexity\n",
    "      val_loss, val_ppl = evaluate(trainer.model, val_loader)\n",
    "      print(\"-\"*100)\n",
    "      print(f\"[Validation, minGPT] iter {trainer.iter_num}: loss {val_loss:.5f}, perplexity {val_ppl:.2f}\")\n",
    "      # generate text\n",
    "      prompt = \"La inteligencia artificial en el mundo moderno\"\n",
    "      samples = generate_with_model(trainer.model, tokenizer, prompt, steps=50, num_samples=1)\n",
    "      print(f\"[Text generation, minGPT] iter {trainer.iter_num}, prompt: {prompt}\")\n",
    "      print('Generation:\\n', samples[0])\n",
    "      print(\"-\"*100)\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535fae9",
   "metadata": {},
   "source": [
    "Train transformed minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8009a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "\n",
    "model_config_tce = GPT_tce.get_default_config()\n",
    "model_config_tce.model_type = model_type\n",
    "model_config_tce.vocab_size = tokenizer.vocab_size\n",
    "model_config_tce.block_size = block_size\n",
    "model_tce = GPT_tce(model_config_tce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26064fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = learning_rate\n",
    "train_config.max_iters = max_iters\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = num_workers\n",
    "\n",
    "trainer_tce = Trainer(train_config, model_tce, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13293929",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_mingpt_tce = []\n",
    "\n",
    "def batch_end_callback_tce(trainer):\n",
    "    if trainer.iter_num % 200 == 0:\n",
    "      train_loss = trainer.loss.item()\n",
    "      train_losses_mingpt_tce.append(train_loss)\n",
    "      perplexity = math.exp(train_loss) if train_loss < 20 else float('inf')\n",
    "      print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {train_loss:.5f}, perplexity {perplexity:.2f}\")\n",
    "    if trainer.iter_num % 2000 == 0:\n",
    "      # val_loss and val_perplexity\n",
    "      val_loss, val_ppl = evaluate(trainer.model, val_loader)\n",
    "      print(\"-\"*100)\n",
    "      print(f\"[Validation, minGPT_tce] iter {trainer.iter_num}: loss {val_loss:.5f}, perplexity {val_ppl:.2f}\")\n",
    "      # generate text\n",
    "      prompt = \"La inteligencia artificial en el mundo moderno\"\n",
    "      samples = generate_with_model(trainer.model, tokenizer, prompt, steps=50, num_samples=1)\n",
    "      print(f\"[Text generation, minGPT_tce] iter {trainer.iter_num}, prompt: {prompt}\")\n",
    "      print('Generation:\\n', samples[0])\n",
    "      print(\"-\"*100)\n",
    "trainer_tce.set_callback('on_batch_end', batch_end_callback_tce)\n",
    "\n",
    "trainer_tce.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75060ed3",
   "metadata": {},
   "source": [
    "Plot training loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570215e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_losses_mingpt, label=\"minGPT\")\n",
    "plt.plot(train_losses_mingpt_tce, label=\"minGPT_tce\")\n",
    "plt.xlabel(\"Checkpoint (every 100 iterations)\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.title(\"Training loss evolution\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jan-gptTransform-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
