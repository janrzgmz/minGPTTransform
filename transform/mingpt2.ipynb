{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ada7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from transform.model_tce import GPT as GPT_tce\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds & Hyperparameters\n",
    "\n",
    "set_seed(3407)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "model_type = 'gpt2-medium' # 345M parameters\n",
    "device = 'cpu'\n",
    "\n",
    "block_size = 1024\n",
    "learning_rate = 3e-4\n",
    "max_iters = 10001\n",
    "batch_size = 512\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b788e4f",
   "metadata": {},
   "source": [
    "GPT2 weights from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained(model_type)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "    if prompt == '':\n",
    "        prompt = '<|endoftext|>'\n",
    "    encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    x = encoded_input['input_ids']\n",
    "    x = x.expand(num_samples, -1)\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "\n",
    "generate(prompt='Artificial intelligence in modern age', num_samples=10, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87a2f5",
   "metadata": {},
   "source": [
    "Training minGPT, model_type: gpt2-medium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.es\")\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42) # 90% training, 10% validation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({'eos_token': ''})\n",
    "\n",
    "print('Length train_dataset: ', len(train_dataset))\n",
    "print('Length val_dataset: ', len(val_dataset))\n",
    "print('vocab_size: ', tokenizer.vocab_size)\n",
    "print(\"Ejemplo train_dataset:\\n\", train_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "\n",
    "tokenizer.model_max_length = int(1e9)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer([t + tokenizer.eos_token for t in examples[\"text\"]], add_special_tokens=False,  return_attention_mask=False)\n",
    "\n",
    "def group_texts_for_minGPT(examples):\n",
    "  concatenated = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "  window = block_size + 1\n",
    "  total_length = (len(concatenated) // window) * window\n",
    "  if total_length == 0:\n",
    "      return {\"input_ids\": [], \"labels\": []}\n",
    "  chunks = [concatenated[i:i+window] for i in range(0, total_length, window)]\n",
    "  inputs = [c[:-1] for c in chunks]\n",
    "  labels = [c[1:]  for c in chunks]\n",
    "  return {\"input_ids\": inputs, \"labels\": labels}\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['id', 'url', 'title', \"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['id', 'url', 'title', \"text\"])\n",
    "\n",
    "lm_train = tokenized_train_dataset.map(group_texts_for_minGPT, batched=True)\n",
    "lm_val = tokenized_val_dataset.map(group_texts_for_minGPT, batched=True)\n",
    "\n",
    "lm_train.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "lm_val.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wikiDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        x = item[\"input_ids\"]\n",
    "        y = item[\"labels\"]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an example instance of the dataset\n",
    "\n",
    "train_dataset = wikiDataset(lm_train)\n",
    "val_dataset = wikiDataset(lm_val)\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "a, b = val_dataset[0]\n",
    "\n",
    "print('x:\\n', x)\n",
    "print('y:\\n', y)\n",
    "print('a:\\n', a)\n",
    "print('b:\\n', b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True,  batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Determine val_loss and perplexity during training\n",
    "def evaluate(model, loader):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float('inf')\n",
    "    if was_training:\n",
    "      model.train()\n",
    "    return avg_loss, ppl\n",
    "\n",
    "# Generate text during training\n",
    "def generate_with_model(model, tokenizer, prompt, steps=50, num_samples=1):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    x = encoded_input['input_ids'].expand(num_samples, -1)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens=steps, do_sample=True, top_k=40)\n",
    "    outputs = []\n",
    "    for i in range(num_samples):\n",
    "        outputs.append(tokenizer.decode(y[i].cpu().squeeze()))\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4cd8b",
   "metadata": {},
   "source": [
    "Train ordinary minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = tokenizer.vocab_size\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = learning_rate\n",
    "train_config.max_iters = max_iters\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = num_workers\n",
    "\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_mingpt = []\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 200 == 0:\n",
    "      train_loss = trainer.loss.item()\n",
    "      train_losses_mingpt.append(train_loss)\n",
    "      perplexity = math.exp(train_loss) if train_loss < 20 else float('inf')\n",
    "      print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {train_loss:.5f}, perplexity {perplexity:.2f}\")\n",
    "    if trainer.iter_num % 2000 == 0:\n",
    "      # val_loss and val_perplexity\n",
    "      val_loss, val_ppl = evaluate(trainer.model, val_loader)\n",
    "      print(\"-\"*100)\n",
    "      print(f\"[Validation, minGPT] iter {trainer.iter_num}: loss {val_loss:.5f}, perplexity {val_ppl:.2f}\")\n",
    "      # generate text\n",
    "      prompt = \"La inteligencia artificial en el mundo moderno\"\n",
    "      samples = generate_with_model(trainer.model, tokenizer, prompt, steps=50, num_samples=1)\n",
    "      print(f\"[Text generation, minGPT] iter {trainer.iter_num}, prompt: {prompt}\")\n",
    "      print('Generation:\\n', samples[0])\n",
    "      print(\"-\"*100)\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535fae9",
   "metadata": {},
   "source": [
    "Train transformed minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8009a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "\n",
    "model_config_tce = GPT_tce.get_default_config()\n",
    "model_config_tce.model_type = model_type\n",
    "model_config_tce.vocab_size = tokenizer.vocab_size\n",
    "model_config_tce.block_size = block_size\n",
    "model_tce = GPT_tce(model_config_tce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26064fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = learning_rate\n",
    "train_config.max_iters = max_iters\n",
    "train_config.batch_size = batch_size\n",
    "train_config.num_workers = num_workers\n",
    "\n",
    "trainer_tce = Trainer(train_config, model_tce, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13293929",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_mingpt_tce = []\n",
    "\n",
    "def batch_end_callback_tce(trainer):\n",
    "    if trainer.iter_num % 200 == 0:\n",
    "      train_loss = trainer.loss.item()\n",
    "      train_losses_mingpt_tce.append(train_loss)\n",
    "      perplexity = math.exp(train_loss) if train_loss < 20 else float('inf')\n",
    "      print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {train_loss:.5f}, perplexity {perplexity:.2f}\")\n",
    "    if trainer.iter_num % 2000 == 0:\n",
    "      # val_loss and val_perplexity\n",
    "      val_loss, val_ppl = evaluate(trainer.model, val_loader)\n",
    "      print(\"-\"*100)\n",
    "      print(f\"[Validation, minGPT_tce] iter {trainer.iter_num}: loss {val_loss:.5f}, perplexity {val_ppl:.2f}\")\n",
    "      # generate text\n",
    "      prompt = \"La inteligencia artificial en el mundo moderno\"\n",
    "      samples = generate_with_model(trainer.model, tokenizer, prompt, steps=50, num_samples=1)\n",
    "      print(f\"[Text generation, minGPT_tce] iter {trainer.iter_num}, prompt: {prompt}\")\n",
    "      print('Generation:\\n', samples[0])\n",
    "      print(\"-\"*100)\n",
    "trainer_tce.set_callback('on_batch_end', batch_end_callback_tce)\n",
    "\n",
    "trainer_tce.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75060ed3",
   "metadata": {},
   "source": [
    "Plot training loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570215e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_losses_mingpt, label=\"minGPT\")\n",
    "plt.plot(train_losses_mingpt_tce, label=\"minGPT-TCE\")\n",
    "plt.xlabel(\"Checkpoint (every 100 iterations)\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.title(\"Training loss evolution\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
